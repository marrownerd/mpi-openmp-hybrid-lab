{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "runtime_attributes": {
        "runtime_version": "2025.10"
      },
      "authorship_tag": "ABX9TyPWVn/P4gus7uq98/BtLzms",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marrownerd/mpi-openmp-hybrid-lab/blob/main/%D0%BB%D0%B0%D0%B1%D0%B02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Устанавливаем библиотеку OpenMPI\n",
        "!apt-get update\n",
        "!apt-get install -y openmpi-bin libopenmpi-dev\n",
        "\n",
        "# 2. Проверяем, что все установилось (должно выдать версию)\n",
        "!mpirun --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZzZi6HOjurp",
        "outputId": "c0c7cf1f-eb07-407b-8aa8-e2cf081967fc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libopenmpi-dev is already the newest version (4.1.2-2ubuntu1).\n",
            "openmpi-bin is already the newest version (4.1.2-2ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 109 not upgraded.\n",
            "mpirun (Open MPI) 4.1.2\n",
            "\n",
            "Report bugs to http://www.open-mpi.org/community/help/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile poisson.cpp\n",
        "#include <mpi.h>\n",
        "#include <omp.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cmath>\n",
        "#include <algorithm>\n",
        "#include <cstdio>\n",
        "\n",
        "// === ПАРАМЕТРЫ ЗАДАЧИ ===\n",
        "const double A_PARAM = 1e5;\n",
        "const double EPS = 1e-8;\n",
        "const double X0 = -1.0, Y0 = -1.0, Z0 = -1.0;\n",
        "const double DX = 2.0, DY = 2.0, DZ = 2.0;\n",
        "\n",
        "// Размер сетки (192 - хорошо для тестов в Colab)\n",
        "const int Nx = 192;\n",
        "const int Ny = 192;\n",
        "const int Nz = 192;\n",
        "\n",
        "// Точное решение и правая часть\n",
        "double phi_exact(double x, double y, double z) { return x*x + y*y + z*z; }\n",
        "double rho_func(double x, double y, double z) { return 6.0 - A_PARAM * phi_exact(x, y, z); }\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    int rank, size, provided;\n",
        "\n",
        "    // Инициализируем MPI с поддержкой многопоточности\n",
        "    MPI_Init_thread(&argc, &argv, MPI_THREAD_FUNNELED, &provided);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    // Шаги по пространству\n",
        "    double hx = DX / (Nx - 1), hy = DY / (Ny - 1), hz = DZ / (Nz - 1);\n",
        "    double hx2 = hx*hx, hy2 = hy*hy, hz2 = hz*hz;\n",
        "    double denom = 2.0/hx2 + 2.0/hy2 + 2.0/hz2 + A_PARAM;\n",
        "\n",
        "    // Одномерная декомпозиция (разрезаем \"тортик\" на слои по оси X)\n",
        "    int local_Nx = Nx / size;\n",
        "    int remainder = Nx % size;\n",
        "    int start_i = rank * local_Nx + std::min(rank, remainder);\n",
        "    if (rank < remainder) local_Nx++;\n",
        "\n",
        "    // Память: локальные слои + 2 теневых (ghost) слоя\n",
        "    size_t layer_size = Ny * Nz;\n",
        "    size_t total_size = (local_Nx + 2) * layer_size;\n",
        "\n",
        "    std::vector<double> phi(total_size, 0.0);\n",
        "    std::vector<double> phi_new(total_size, 0.0);\n",
        "    std::vector<double> rho_arr(total_size, 0.0);\n",
        "\n",
        "    // --- 1. Инициализация ---\n",
        "    // i идет от 0 до local_Nx+1 (локальная индексация)\n",
        "    #pragma omp parallel for collapse(2)\n",
        "    for (int i = 0; i < local_Nx + 2; ++i) {\n",
        "        for (int j = 0; j < Ny; ++j) {\n",
        "            for (int k = 0; k < Nz; ++k) {\n",
        "                // Перевод в глобальную координату\n",
        "                int global_i = start_i + (i - 1);\n",
        "                double x = X0 + global_i * hx;\n",
        "                double y = Y0 + j * hy;\n",
        "                double z = Z0 + k * hz;\n",
        "\n",
        "                int idx = i * layer_size + j * Nz + k;\n",
        "                rho_arr[idx] = rho_func(x, y, z);\n",
        "\n",
        "                // Граничные условия (фиксируем значения на краях куба)\n",
        "                if (global_i == 0 || global_i == Nx - 1 || j == 0 || j == Ny - 1 || k == 0 || k == Nz - 1) {\n",
        "                    phi[idx] = phi_exact(x, y, z);\n",
        "                    phi_new[idx] = phi[idx];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    double max_diff = 0.0;\n",
        "    int it = 0;\n",
        "    double start_time = MPI_Wtime();\n",
        "\n",
        "    // --- 2. Основной цикл ---\n",
        "    do {\n",
        "        it++;\n",
        "        max_diff = 0.0;\n",
        "        MPI_Request reqs[4];\n",
        "        int n_reqs = 0;\n",
        "\n",
        "        // Асинхронный обмен границами (Halo exchange)\n",
        "        // Отправляем свои крайние слои соседям\n",
        "        if (rank > 0) {\n",
        "            MPI_Isend(&phi[1 * layer_size], layer_size, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD, &reqs[n_reqs++]);\n",
        "            MPI_Irecv(&phi[0 * layer_size], layer_size, MPI_DOUBLE, rank - 1, 1, MPI_COMM_WORLD, &reqs[n_reqs++]);\n",
        "        }\n",
        "        if (rank < size - 1) {\n",
        "            MPI_Isend(&phi[local_Nx * layer_size], layer_size, MPI_DOUBLE, rank + 1, 1, MPI_COMM_WORLD, &reqs[n_reqs++]);\n",
        "            MPI_Irecv(&phi[(local_Nx + 1) * layer_size], layer_size, MPI_DOUBLE, rank + 1, 0, MPI_COMM_WORLD, &reqs[n_reqs++]);\n",
        "        }\n",
        "\n",
        "        // Лямбда-функция для расчета одного слоя\n",
        "        auto calc_layer = [&](int i) {\n",
        "            int global_i = start_i + (i - 1);\n",
        "            // Считаем только внутренние точки (границы фиксированы)\n",
        "            if (global_i > 0 && global_i < Nx - 1) {\n",
        "                double local_diff = 0.0;\n",
        "\n",
        "                // Параллелим циклы по Y и Z с помощью OpenMP\n",
        "                #pragma omp parallel for collapse(2) reduction(max: local_diff)\n",
        "                for(int j=1; j<Ny-1; ++j) {\n",
        "                    for(int k=1; k<Nz-1; ++k) {\n",
        "                        int idx = i*layer_size + j*Nz + k;\n",
        "                        // Формула Якоби (крест)\n",
        "                        double val = ((phi[idx+layer_size] + phi[idx-layer_size])/hx2 +\n",
        "                                      (phi[idx+Nz] + phi[idx-Nz])/hy2 +\n",
        "                                      (phi[idx+1] + phi[idx-1])/hz2 - rho_arr[idx]) / denom;\n",
        "                        phi_new[idx] = val;\n",
        "                        local_diff = std::max(local_diff, std::abs(val - phi[idx]));\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                // Обновляем общий максимум (безопасно для потоков)\n",
        "                #pragma omp critical\n",
        "                {\n",
        "                    if(local_diff > max_diff) max_diff = local_diff;\n",
        "                }\n",
        "            }\n",
        "        };\n",
        "\n",
        "        // (А) Считаем внутреннюю часть области (пока идут обмены)\n",
        "        if (local_Nx > 2) {\n",
        "            for (int i = 2; i < local_Nx; ++i) calc_layer(i);\n",
        "        }\n",
        "\n",
        "        // (Б) Ждем завершения обменов\n",
        "        MPI_Waitall(n_reqs, reqs, MPI_STATUSES_IGNORE);\n",
        "\n",
        "        // (В) Считаем границы (теперь у нас есть данные от соседей)\n",
        "        if (local_Nx >= 1) calc_layer(1);\n",
        "        if (local_Nx > 1) calc_layer(local_Nx);\n",
        "\n",
        "        // Обновляем массив\n",
        "        std::swap(phi, phi_new);\n",
        "\n",
        "        // Собираем максимальную ошибку со всех процессов\n",
        "        double global_max_diff;\n",
        "        MPI_Allreduce(&max_diff, &global_max_diff, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);\n",
        "        max_diff = global_max_diff;\n",
        "\n",
        "    } while (max_diff > EPS && it < 10000); // Ограничим 10000 итераций на всякий случай\n",
        "\n",
        "    double end_time = MPI_Wtime();\n",
        "\n",
        "    // --- 3. Проверка результата (считаем отклонение от точного решения) ---\n",
        "    double local_err = 0.0;\n",
        "    #pragma omp parallel for collapse(2) reduction(max: local_err)\n",
        "    for(int i=1; i<=local_Nx; ++i) {\n",
        "        for(int j=0; j<Ny; ++j) {\n",
        "            for(int k=0; k<Nz; ++k) {\n",
        "                int global_i = start_i + (i - 1);\n",
        "                double x = X0 + global_i*hx, y = Y0 + j*hy, z = Z0 + k*hz;\n",
        "                local_err = std::max(local_err, std::abs(phi[i*layer_size + j*Nz + k] - phi_exact(x,y,z)));\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    double global_err;\n",
        "    MPI_Reduce(&local_err, &global_err, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);\n",
        "\n",
        "    if (rank == 0) {\n",
        "        printf(\"RESULT: MPI=%d, OMP=%d, Time=%.4f sec, Error=%.2e, Iterations=%d\\n\",\n",
        "               size, omp_get_max_threads(), end_time - start_time, global_err, it);\n",
        "    }\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRYboySZjyH8",
        "outputId": "fd1493d0-e4ce-4754-f82f-27d18e172987"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting poisson.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Компилируем с флагами оптимизации (-O3) и поддержкой OpenMP (-fopenmp)\n",
        "!mpicxx poisson.cpp -o poisson_run -fopenmp -O3\n",
        "\n",
        "# Проверяем, создался ли файл (должен быть в списке)\n",
        "!ls -l poisson_run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNc5CkEXj4wT",
        "outputId": "311e1d9c-e4ca-4c81-9df9-35daf39dd5e2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rwxr-xr-x 1 root root 104632 Dec 25 08:11 poisson_run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Функция запуска одного теста\n",
        "def run_experiment(mpi_procs, omp_threads):\n",
        "    print(f\"⏳ Тест: {mpi_procs} MPI процесс(а) x {omp_threads} поток(а)...\", end=\"\", flush=True)\n",
        "\n",
        "    env = os.environ.copy()\n",
        "    env[\"OMP_NUM_THREADS\"] = str(omp_threads)\n",
        "\n",
        "    # Команда запуска MPI\n",
        "    cmd = [\n",
        "        \"mpirun\",\n",
        "        \"--allow-run-as-root\",  # Разрешить запуск от root (нужно для Colab)\n",
        "        \"--oversubscribe\",      # Разрешить запускать больше процессов, чем есть ядер\n",
        "        \"-np\", str(mpi_procs),\n",
        "        \"./poisson_run\"\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        result = subprocess.run(cmd, env=env, capture_output=True, text=True, timeout=120)\n",
        "        if result.returncode == 0:\n",
        "            # Ищем строку результата в выводе\n",
        "            for line in result.stdout.split('\\n'):\n",
        "                if \"RESULT:\" in line:\n",
        "                    print(\" ✅ Готово!\")\n",
        "                    print(\"   >> \" + line)\n",
        "                    return\n",
        "            print(\" ⚠️ Завершилось, но результат не найден.\")\n",
        "        else:\n",
        "            print(\" ❌ Ошибка!\")\n",
        "            print(result.stderr)\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\" ⏱️ Долго! (Таймаут)\")\n",
        "\n",
        "# === ЗАПУСК ЭКСПЕРИМЕНТОВ ===\n",
        "\n",
        "print(\"\\n--- ЭТАП 1: Масштабируемость (Только MPI) ---\")\n",
        "# В Colab всего 2 ядра, поэтому ускорение будет не идеальным, но цифры мы получим\n",
        "for p in [1, 2, 4]:\n",
        "    run_experiment(p, 1)\n",
        "\n",
        "print(\"\\n--- ЭТАП 2: Гибридный режим (Поиск лучшей конфигурации) ---\")\n",
        "# Эмулируем работу на \"4 узлах\" (всего 4 потока вычислений)\n",
        "configs = [\n",
        "    (1, 4), # 1 процесс, 4 потока\n",
        "    (2, 2), # 2 процесса по 2 потока\n",
        "    (4, 1)  # 4 процесса по 1 потоку\n",
        "]\n",
        "\n",
        "for mpi, omp in configs:\n",
        "    run_experiment(mpi, omp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgSSVhVMkECk",
        "outputId": "f6bf74a3-6542-43e1-9f60-8ea88c1d4588"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ЭТАП 1: Масштабируемость (Только MPI) ---\n",
            "⏳ Тест: 1 MPI процесс(а) x 1 поток(а)... ✅ Готово!\n",
            "   >> RESULT: MPI=1, OMP=1, Time=0.7703 sec, Error=2.36e-09, Iterations=20\n",
            "⏳ Тест: 2 MPI процесс(а) x 1 поток(а)... ✅ Готово!\n",
            "   >> RESULT: MPI=2, OMP=1, Time=1.3199 sec, Error=2.36e-09, Iterations=20\n",
            "⏳ Тест: 4 MPI процесс(а) x 1 поток(а)... ✅ Готово!\n",
            "   >> RESULT: MPI=4, OMP=1, Time=1.0849 sec, Error=2.36e-09, Iterations=20\n",
            "\n",
            "--- ЭТАП 2: Гибридный режим (Поиск лучшей конфигурации) ---\n",
            "⏳ Тест: 1 MPI процесс(а) x 4 поток(а)... ✅ Готово!\n",
            "   >> RESULT: MPI=1, OMP=4, Time=0.8720 sec, Error=2.36e-09, Iterations=20\n",
            "⏳ Тест: 2 MPI процесс(а) x 2 поток(а)... ✅ Готово!\n",
            "   >> RESULT: MPI=2, OMP=2, Time=2.3890 sec, Error=2.36e-09, Iterations=20\n",
            "⏳ Тест: 4 MPI процесс(а) x 1 поток(а)... ✅ Готово!\n",
            "   >> RESULT: MPI=4, OMP=1, Time=0.8624 sec, Error=2.36e-09, Iterations=20\n"
          ]
        }
      ]
    }
  ]
}